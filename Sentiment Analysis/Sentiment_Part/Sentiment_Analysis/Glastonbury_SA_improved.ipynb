{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mColumns: Sentiment, Id, Date, Flag, User, Tweet\u001b[0m\n",
      "\u001b[33mUseful columns: Sentiment and Tweet\u001b[0m\n",
      "\u001b[31mRemoving other columns\u001b[0m\n",
      "\u001b[31mColumns removed\u001b[0m\n",
      "\u001b[33mSplitting train and test dataset into 80:20\u001b[0m\n",
      "\u001b[33mTrain data distribution:\u001b[0m\n",
      "0    10875\n",
      "1     5466\n",
      "Name: Sentiment, dtype: int64\n",
      "\u001b[33mTest data distribution:\u001b[0m\n",
      "0    2653\n",
      "1    1433\n",
      "Name: Sentiment, dtype: int64\n",
      "\u001b[33mSplit complete\u001b[0m\n",
      "\u001b[33mSaving train data\u001b[0m\n",
      "\u001b[32mTrain data saved to data/train.csv\u001b[0m\n",
      "\u001b[33mSaving test data\u001b[0m\n",
      "\u001b[32mTest data saved to data/test.csv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define variables\n",
    "COLUMNS = ['Sentiment', 'Id', 'Date', 'Flag', 'User', 'Tweet']\n",
    "\n",
    "# Read dataset\n",
    "\n",
    "dataset = pd.read_csv('C://Users/44740/Machine learning/Glastonbury_Improved/Glastonbury_Improved/Sentiment_analysis_AD_20.csv', names = COLUMNS, encoding = 'latin-1')\n",
    "dataset.head()\n",
    "\n",
    "\n",
    "print(colored(\"Columns: {}\".format(', '.join(COLUMNS)), \"yellow\"))\n",
    "\n",
    "# Remove extra columns\n",
    "print(colored(\"Useful columns: Sentiment and Tweet\", \"yellow\"))\n",
    "print(colored(\"Removing other columns\", \"red\"))\n",
    "dataset.drop(['Id', 'Date', 'Flag', 'User'], axis = 1, inplace = True)\n",
    "print(colored(\"Columns removed\", \"red\"))\n",
    "\n",
    "# Train test split\n",
    "print(colored(\"Splitting train and test dataset into 80:20\", \"yellow\"))\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['Tweet'], dataset['Sentiment'], test_size = 0.20, random_state = 100)\n",
    "train_dataset = pd.DataFrame({\n",
    "\t'Tweet': X_train,\n",
    "\t'Sentiment': y_train\n",
    "\t})\n",
    "print(colored(\"Train data distribution:\", \"yellow\"))\n",
    "print(train_dataset['Sentiment'].value_counts())\n",
    "test_dataset = pd.DataFrame({\n",
    "\t'Tweet': X_test,\n",
    "\t'Sentiment': y_test\n",
    "\t})\n",
    "print(colored(\"Test data distribution:\", \"yellow\"))\n",
    "print(test_dataset['Sentiment'].value_counts())\n",
    "print(colored(\"Split complete\", \"yellow\"))\n",
    "\n",
    "# Save train data\n",
    "print(colored(\"Saving train data\", \"yellow\"))\n",
    "\n",
    "train_dataset.to_csv('data/train.csv', index = False)\n",
    "print(colored(\"Train data saved to data/train.csv\", \"green\"))\n",
    "\n",
    "# Save test data\n",
    "print(colored(\"Saving test data\", \"yellow\"))\n",
    "test_dataset.to_csv('data/test.csv', index = False)\n",
    "print(colored(\"Test data saved to data/test.csv\", \"green\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\44740\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\44740\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "\u001b[32mProcessing train data\u001b[0m\n",
      "\u001b[33mRemoving user handles starting with @\u001b[0m\n",
      "\u001b[34mRemoving numbers and special characters\u001b[0m\n",
      "\u001b[32mRemoving urls\u001b[0m\n",
      "\u001b[33mRemoving single characters\u001b[0m\n",
      "\u001b[33mTokenizing\u001b[0m\n",
      "\u001b[33mRemoving stopwords\u001b[0m\n",
      "\u001b[33mExpanding not words\u001b[0m\n",
      "\u001b[33mLemmatizing the words\u001b[0m\n",
      "\u001b[33mStemming the words\u001b[0m\n",
      "\u001b[33mCombining words back to tweets\u001b[0m\n",
      "\u001b[32mTrain data processed and saved to data/clean_train.csv\u001b[0m\n",
      "\u001b[32mProcessing test data\u001b[0m\n",
      "\u001b[33mRemoving user handles starting with @\u001b[0m\n",
      "\u001b[34mRemoving numbers and special characters\u001b[0m\n",
      "\u001b[32mRemoving urls\u001b[0m\n",
      "\u001b[33mRemoving single characters\u001b[0m\n",
      "\u001b[33mTokenizing\u001b[0m\n",
      "\u001b[33mRemoving stopwords\u001b[0m\n",
      "\u001b[33mExpanding not words\u001b[0m\n",
      "\u001b[33mLemmatizing the words\u001b[0m\n",
      "\u001b[33mStemming the words\u001b[0m\n",
      "\u001b[33mCombining words back to tweets\u001b[0m\n",
      "\u001b[32mTest data processed and saved to data/clean_test.csv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from termcolor import colored\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import datasets\n",
    "print(\"Loading data\")\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Setting stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS.remove(\"not\")\n",
    "\n",
    "# Function to expand tweet\n",
    "def expand_tweet(tweet):\n",
    "\texpanded_tweet = []\n",
    "\tfor word in tweet:\n",
    "\t\tif re.search(\"n't\", word):\n",
    "\t\t\texpanded_tweet.append(word.split(\"n't\")[0])\n",
    "\t\t\texpanded_tweet.append(\"not\")\n",
    "\t\telse:\n",
    "\t\t\texpanded_tweet.append(word)\n",
    "\treturn expanded_tweet\n",
    "\n",
    "# Function to process tweets\n",
    "def clean_tweet(data, wordNetLemmatizer, porterStemmer):\n",
    "\tdata['Clean_tweet'] = data['Tweet'] # Saving data\n",
    "\tprint(colored(\"Removing user handles starting with @\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.replace(\"@[\\w]*\",\"\") # Removing user handles\n",
    "\tprint(colored(\"Removing numbers and special characters\", \"blue\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.replace(\"[^a-zA-Z'#]\",\" \") # Removing numbers and special characters\n",
    "\tprint(colored(\"Removing urls\", \"green\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\"), \" \") # Removing URLs\n",
    "\tprint(colored(\"Removing single characters\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"(^| ).( |$)\"), \" \") # Removing single characters\n",
    "\tprint(colored(\"Tokenizing\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.split() # Tokenizing\n",
    "\tprint(colored(\"Removing stopwords\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].dropna().apply(lambda x: [item for item in x if item not in STOPWORDS]) #data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [word for word in tweet if word not in STOPWORDS]) # Removing stopwords\n",
    "\tprint(colored(\"Expanding not words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].dropna().apply(lambda tweet: expand_tweet(tweet)) # Expanding NOT words\n",
    "\tprint(colored(\"Lemmatizing the words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].dropna().apply(lambda tweet: [wordNetLemmatizer.lemmatize(word) for word in tweet]) # Lemmatizing\n",
    "\tprint(colored(\"Stemming the words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].dropna().apply(lambda tweet: [porterStemmer.stem(word) for word in tweet]) # Stemming\n",
    "\tprint(colored(\"Combining words back to tweets\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].dropna().apply(lambda tweet: ' '.join(tweet))\n",
    "\treturn data\n",
    "\n",
    "# Define processing methods\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "# Pre-processing the tweets\n",
    "print(colored(\"Processing train data\", \"green\"))\n",
    "train_data = clean_tweet(train_data, wordNetLemmatizer, porterStemmer)\n",
    "train_data.to_csv('data/clean_train.csv', index = False)\n",
    "print(colored(\"Train data processed and saved to data/clean_train.csv\", \"green\"))\n",
    "print(colored(\"Processing test data\", \"green\"))\n",
    "test_data = clean_tweet(test_data, wordNetLemmatizer, porterStemmer)\n",
    "test_data.to_csv('data/clean_test.csv', index = False)\n",
    "print(colored(\"Test data processed and saved to data/clean_test.csv\", \"green\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLoading train and test data\u001b[0m\n",
      "\u001b[33mData loaded\u001b[0m\n",
      "\u001b[33mTokenizing and padding data\u001b[0m\n",
      "\u001b[33mTokenizing and padding complete\u001b[0m\n",
      "\u001b[33mCreating the LSTM model\u001b[0m\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 650,754\n",
      "Trainable params: 650,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\u001b[32mTraining the LSTM model\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13072 samples, validate on 3269 samples\n",
      "Epoch 1/20\n",
      "13072/13072 [==============================] - 13s 992us/step - loss: 0.5993 - accuracy: 0.6735 - val_loss: 0.5562 - val_accuracy: 0.6837\n",
      "Epoch 2/20\n",
      "13072/13072 [==============================] - 12s 910us/step - loss: 0.5375 - accuracy: 0.7013 - val_loss: 0.5574 - val_accuracy: 0.6852\n",
      "Epoch 3/20\n",
      "13072/13072 [==============================] - 12s 901us/step - loss: 0.5095 - accuracy: 0.7255 - val_loss: 0.5611 - val_accuracy: 0.6757\n",
      "Epoch 4/20\n",
      "13072/13072 [==============================] - 12s 911us/step - loss: 0.4911 - accuracy: 0.7330 - val_loss: 0.5671 - val_accuracy: 0.6788\n",
      "Epoch 5/20\n",
      "13072/13072 [==============================] - 12s 887us/step - loss: 0.4749 - accuracy: 0.7477 - val_loss: 0.5975 - val_accuracy: 0.6757\n",
      "Epoch 6/20\n",
      "13072/13072 [==============================] - 12s 915us/step - loss: 0.4592 - accuracy: 0.7599 - val_loss: 0.5911 - val_accuracy: 0.6696\n",
      "Epoch 7/20\n",
      "13072/13072 [==============================] - 12s 881us/step - loss: 0.4467 - accuracy: 0.7679 - val_loss: 0.6113 - val_accuracy: 0.6647\n",
      "Epoch 8/20\n",
      "13072/13072 [==============================] - 11s 861us/step - loss: 0.4351 - accuracy: 0.7733 - val_loss: 0.6178 - val_accuracy: 0.6598\n",
      "Epoch 9/20\n",
      "13072/13072 [==============================] - 12s 888us/step - loss: 0.4250 - accuracy: 0.7812 - val_loss: 0.6678 - val_accuracy: 0.6565\n",
      "Epoch 10/20\n",
      "13072/13072 [==============================] - 12s 887us/step - loss: 0.4100 - accuracy: 0.7876 - val_loss: 0.7264 - val_accuracy: 0.6568\n",
      "Epoch 11/20\n",
      "13072/13072 [==============================] - 11s 879us/step - loss: 0.4005 - accuracy: 0.7888 - val_loss: 0.6910 - val_accuracy: 0.6504\n",
      "Epoch 12/20\n",
      "13072/13072 [==============================] - 12s 888us/step - loss: 0.3905 - accuracy: 0.7971 - val_loss: 0.7266 - val_accuracy: 0.6436\n",
      "Epoch 13/20\n",
      "13072/13072 [==============================] - 12s 891us/step - loss: 0.3854 - accuracy: 0.8039 - val_loss: 0.7608 - val_accuracy: 0.6494\n",
      "Epoch 14/20\n",
      "13072/13072 [==============================] - 11s 876us/step - loss: 0.3721 - accuracy: 0.8055 - val_loss: 0.7581 - val_accuracy: 0.6580\n",
      "Epoch 15/20\n",
      "13072/13072 [==============================] - 12s 888us/step - loss: 0.3622 - accuracy: 0.8135 - val_loss: 0.8269 - val_accuracy: 0.6528\n",
      "Epoch 16/20\n",
      "13072/13072 [==============================] - 12s 940us/step - loss: 0.3552 - accuracy: 0.8195 - val_loss: 0.7661 - val_accuracy: 0.6406\n",
      "Epoch 17/20\n",
      "13072/13072 [==============================] - 13s 1ms/step - loss: 0.3504 - accuracy: 0.8228 - val_loss: 0.8308 - val_accuracy: 0.6452\n",
      "Epoch 18/20\n",
      "13072/13072 [==============================] - 12s 886us/step - loss: 0.3420 - accuracy: 0.8222 - val_loss: 0.8530 - val_accuracy: 0.6406\n",
      "Epoch 19/20\n",
      "13072/13072 [==============================] - 11s 865us/step - loss: 0.3355 - accuracy: 0.8292 - val_loss: 0.8748 - val_accuracy: 0.6424\n",
      "Epoch 20/20\n",
      "13072/13072 [==============================] - 11s 879us/step - loss: 0.3279 - accuracy: 0.8315 - val_loss: 0.9110 - val_accuracy: 0.6400\n",
      "\u001b[32m<keras.callbacks.callbacks.History object at 0x0000016B77A2DF88>\u001b[0m\n",
      "\u001b[32mTesting the LSTM model\u001b[0m\n",
      "4086/4086 [==============================] - 1s 217us/step\n",
      "Test accuracy: 0.624571681022644\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "# Load data\n",
    "print(colored(\"Loading train and test data\", \"yellow\"))\n",
    "train_data = pd.read_csv('data/clean_train.csv')\n",
    "test_data = pd.read_csv('data/clean_test.csv')\n",
    "print(colored(\"Data loaded\", \"yellow\"))\n",
    "\n",
    "# Tokenization\n",
    "print(colored(\"Tokenizing and padding data\", \"yellow\"))\n",
    "tokenizer = Tokenizer(num_words = 2000, split = ' ')\n",
    "tokenizer.fit_on_texts(train_data['Clean_tweet'].astype(str).values)\n",
    "train_tweets = tokenizer.texts_to_sequences(train_data['Clean_tweet'].astype(str).values)\n",
    "max_len = max([len(i) for i in train_tweets])\n",
    "train_tweets = pad_sequences(train_tweets, maxlen = max_len)\n",
    "test_tweets = tokenizer.texts_to_sequences(test_data['Clean_tweet'].astype(str).values)\n",
    "test_tweets = pad_sequences(test_tweets, maxlen = max_len)\n",
    "print(colored(\"Tokenizing and padding complete\", \"yellow\"))\n",
    "\n",
    "# Building the model\n",
    "print(colored(\"Creating the LSTM model\", \"yellow\"))\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 128, input_length = train_tweets.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(256, dropout = 0.2))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "print(colored(\"Training the LSTM model\", \"green\"))\n",
    "history = model.fit(train_tweets, pd.get_dummies(train_data['Sentiment']).values, epochs = 20, batch_size = 128, validation_split = 0.2)\n",
    "print(colored(history, \"green\"))\n",
    "\n",
    "# Testing the model\n",
    "print(colored(\"Testing the LSTM model\", \"green\"))\n",
    "score, accuracy = model.evaluate(test_tweets, pd.get_dummies(test_data['Sentiment']).values, batch_size = 128)\n",
    "print(\"Test accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
